---
title: "Práctica 2 - Limpieza y validación de datos"
author: "Rubén Salamanqués y Ricardo Pardo"
date: "10 de junio de 2018"
output:
  html_document:
    toc: true
    theme: united
    df_print: paged
    
#csl: apa.csl
#bibliography: bibliography.bib    
---
- - -

```{r setup, include=FALSE}
library('knitr')
library('kableExtra')
#knitr::opts_chunk$set(echo = TRUE)

#Opciones
#options(digits = 3)
```
##Introducción

El objetivo de esta actividad será el tratamiento de un dataset, que puede ser el creado en la práctica 1 o bien cualquier dataset libre disponible en Kaggle (https://www.kaggle.com). Algunos ejemplos de dataset con los que podéis trabajar son:

* Red Wine Quality (https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)
* Titanic: Machine Learning from Disaster (https://www.kaggle.com/c/titanic)
* Predict Future Sales (https://www.kaggle.com/c/competitive-data-sciencepredict-future-sales/).

Los últimos dos ejemplos corresponden a competiciones activas de Kaggle de manera que, opcionalmente, podríais aprovechar el trabajo realizado durante la práctica para entrar en alguna de estas competiciones.

Para llevar a cabo el trabajo de esta práctica, hemos escogido el dataset "Global Commodity Trade Statistics", que se encuentra disponible en: https://www.kaggle.com/unitednations/global-commodity-trade-statistics/data


A continuación, siguiendo las principales etapas de un proyecto analítico, las diferentes tareas a realizar (y justificar) son las siguientes:


- - -

```{r, include=FALSE, results='hold'}
#cargamos el fichero de datos World Happiness Report limpio, vemos que los separadores son comas y que tiene cabecera


# lectura del fichero con datos a partir de 2006
cmm_trade_data = read.csv("C:\Users\Ruben\Google Drive\UOC\2_Tipología y Ciclo de Vida de los Datos\Prácticas\Práctica2\commodity_trade_statistics_data_2006.csv", head=TRUE, sep = ",")

#mostramos las primeras filas del fichero
#head(cmm_trade_data,10)
```

- - -

##1.- Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?

Se trata de un dataset originalmente publicado por "United Nations Statistics Division" en la página de UNData (http://data.un.org/Explorer.aspx). Los términos de uso dicen que todos los datos y metadatos provenientes de la página de UNData pueden ser utilizados sin coste y pueden ser copiados libremente y distribuidos posteriormente siempre y cuando se cite a UNdata como fuente original.

El dataset contiene información sobre flujos de exportación e importación de animales y productos de consumo de países del mundo.


A partir de estos datos se pueden llevar a cabo estudios sobre los flujos de exportaciones e importaciones de los países........................blah blah blah 


Obtenemos un resumen de los datos para llevar a cabo un análisis previo:
```{r, include=TRUE, echo=FALSE, results='hold'}
#resumen de los datos
summary(cmm_trade_data)
```

Llevamos a cabo un análisis "manual" de la información. Vemos que para el campo "comm_code" tenemos dos valores especiales: "999999", que significa "Commodities not specified according to kind" y "TOTAL", que representa, por país y año, el total en USD de cada uno de los posibles flujos (exportaciones, importaciones, re-exportaciones y re-importaciones).

En cuanto a la variable "category", vemos que se trata de una cadena que concatena dos dígitos y un texto; para su tratamiento, separaremos los dos dígitos del resto. Hemos visto un valor especial, se trata de "all_commodities", se trata del valor que se carga para "comm_code=TOTAL", por tanto, para estas observaciones cargaremos un NA.



A continuación, mostramos diagramas de caja para los valores de cada una de las columnas del archivo:
```{r, include=TRUE, echo=FALSE, results='hold'}
##Gr?ficos
# boxplot(whr_data[, c("HS", "LCI", "UCI")], col="grey", xlab='', ylab='', horizontal=TRUE)
# boxplot(whr_data[, c("GpC", "Family", "LE", "DR")], col="grey", xlab='', ylab='', horizontal=TRUE)
# boxplot(whr_data[, c("Freedom", "GC", "Generosity")], col="grey", xlab='', ylab='', horizontal=TRUE)
```


blah blah

- - -

##2.- Integración y selección de los datos de interés a analizar.

blah blah

- - -

##3.- Limpieza de los datos

Tal y como hemos visto en el punto 1, los datos necesitan una limpieza previa para poder llevar a cabo los análisis.....



```{r, include=TRUE, echo=FALSE, results='hold'}
## Creamos un dataframe con la informaci?n sobre c?digo de categor?a - descripci?n

categories = cmm_trade_data[c("category")]
categories = subset(categories, category != "all_commodities" | category == "NA")
categories = categories[!duplicated(categories$category), ]
categories = data.frame(categories)
categories = data.frame("category"=categories)
colnames(categories)[which(names(categories) == "categories")] <- "category"

# de esta forma crear?amos una tabla con frecuencias
# categories <- as.data.frame(ftable(categories$category))

# separamos el c?digo de la descripci?n
categories$category_number <- substring(categories$category, 1, 2)
categories$category_desc <- substring(categories$category, 4)
# almacenamos el c?digo como num?rico
categories$category_number <- as.numeric(unlist(categories$category_number))

## Actualizamos los valores de "category" del dataset de origen
# para las filas con category=all_commodities, dejamos un NA
cmm_trade_data[which(cmm_trade_data$category=='all_commodities'),]$category <- "NA"
# sustituimos el valor por la clave num?rica
cmm_trade_data$category = substring(cmm_trade_data$category, 1, 2)
cmm_trade_data$category <- as.numeric(unlist(cmm_trade_data$category))

```



###3.1.- ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos?



###3.2.- Identificación y tratamiento de valores extremos.

- - -

##4.- Análisis de los datos.

###4.1.- Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).

###4.2.- Comprobación de la normalidad y homogeneidad de la varianza.

###4.3.- Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc.

- - -

##5.- Representación de los resultados a partir de tablas y gráficas.

- - -

##6.- Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?

- - -

##7.- Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar en Python.

- - -

##Criterios de valoración

Todos los apartados son obligatorios. La ponderación de los ejercicios es la siguiente:

* Los apartados 1, 2 y 6 valen 0,5 puntos.
* Los apartados 3, 5 y 7 valen 2 puntos.
* El apartado 4 vale 2,5 puntos.

Se valorará la idoneidad de las respuestas, que deberán ser claras y completas. Las
diferentes etapas deberán justificarse y acompañarse del código correspondiente.

También se valorará la síntesis y claridad, a través del uso de comentarios, del código
resultante, así como la calidad de los datos finales analizados.

- - -
- - -
