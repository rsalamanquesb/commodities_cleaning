---
title: "Práctica 2 - Limpieza y validación de datos"
author: "Rubén Salamanqués y Ricardo Pardo"
date: "10 de junio de 2018"
output:
  html_document:
    toc: true
    #theme: united
    df_print: paged
    
#csl: apa.csl
#bibliography: bibliography.bib    
---
- - -

```{r setup, include=FALSE}
library('knitr')
library('kableExtra')
knitr::opts_chunk$set(echo = TRUE)

```
##Introducción

El objetivo de esta actividad será el tratamiento de un dataset, que puede ser el creado en la práctica 1 o bien cualquier dataset libre disponible en Kaggle (https://www.kaggle.com). Algunos ejemplos de dataset con los que podéis trabajar son:

* Red Wine Quality (https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)
* Titanic: Machine Learning from Disaster (https://www.kaggle.com/c/titanic)
* Predict Future Sales (https://www.kaggle.com/c/competitive-data-sciencepredict-future-sales/).

Los últimos dos ejemplos corresponden a competiciones activas de Kaggle de manera que, opcionalmente, podríais aprovechar el trabajo realizado durante la práctica para entrar en alguna de estas competiciones.

Para llevar a cabo el trabajo de esta práctica, hemos escogido el dataset "Global Commodity Trade Statistics", que se encuentra disponible en: https://www.kaggle.com/unitednations/global-commodity-trade-statistics/data


A continuación, siguiendo las principales etapas de un proyecto analítico, las diferentes tareas a realizar (y justificar) son las siguientes:


- - -

##1.- Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?

Se trata de un dataset originalmente publicado por "United Nations Statistics Division" en la página de UNData (http://data.un.org/Explorer.aspx). Los términos de uso dicen que todos los datos y metadatos provenientes de la página de UNData pueden ser utilizados sin coste y pueden ser copiados libremente y distribuidos posteriormente siempre y cuando se cite a UNdata como fuente original.

El dataset contiene información sobre flujos de exportación e importación de animales y productos de consumo de países del mundo durante un periodo de 30 años.

Debido al gran volumen que esto supone se va a trabajar con un subconjunto que comprende el periodo entre los años 2006 y 2016.

A partir de estos datos se pueden llevar a cabo estudios sobre los flujos de exportaciones e importaciones de los países en un periodo en el que se ha producido una de las mayores crisis a nivel mundial del último siglo.

  * ¿Se ha reducido el volumen comercial (en $) en este periodo para los países del G20?

  * ¿Cuál será la previsión de exportación de la categoría más exportada por España?

  * ¿El mercado chino ha resistido mejor la crisis en comparación con el estadounidense?

Las columnas del dataset son las siguientes:

  * **country_or_area:** País o zona geográfica.
  * **year:** Año de la transacción.
  * **comm_code:** Código de la mercancía.
  * **commodity:** Descripción de la mercancía.
  * **flow:** Flujo de la transacción (importación/exportación/re-importación/re-exportación).
  * **trade_usd:** Precio de la transacción (en dólares estadounidenses).
  * **weight_kg:** Peso en kilogramos.
  * **quantity_name:** Magnitud de la cantidad.
  * **quantity:** Cantidad
  * **category:** Categoría genérica de la transacción

- - -

##2.- Integración y selección de los datos de interés a analizar.

Como el archivo original es muy grande, se ha preparado una versión que contiene únicamente los datos a partir del año 2006. Con el siguiente código se carga dicho archivo.

```{r, include=FALSE, results='hold'}

# lectura del fichero con datos a partir de 2006
#cmm_trade_data = read.csv("C:/Users/Ruben/Google Drive/UOC/2_Tipología y Ciclo de Vida de los Datos/Prácticas/Práctica2/commodity_trade_statistics_data_2006.csv", head=TRUE, sep = ",")

cmm_trade_data = read.csv("C:/Users/Rsb/Google Drive/UOC/2_Tipología y Ciclo de Vida de los Datos/Prácticas/Práctica2/commodity_trade_statistics_data_2006.csv", head=TRUE, sep = ",")

```

Con esto se obtiene un conjunto de datos con las características explicadas anteriormente. Sin embargo, para responder a las cuestiones planteadas, tan solo es necesario trabajar con las siguientes columnas.


  * **country_or_area:** Para hacer análisis por países.
  * **year:** Para estudiar la evolución temporal.
  * **comm_code:** Para corregir datos ausentes en otros campos.
  * **flow:** Para estudiar solo ciertos flujos.
  * **trade_usd:** Para analizar los volúmenes de las transacciones.
  * **category:** Para agrupar por categorías.
  
Además, se va a trabajar únicamente con los países del G-20 (incluyendo a España en lugar de la UE en el último puesto).

El siguiente código en R proporciona un subconjunto de los datos cargados que cumple los requisitos expuestos.

```{r, include=FALSE, results='hold'}

cmm_trade_data = read.csv("C:/Users/Rsb/Google Drive/UOC/2_Tipología y Ciclo de Vida de los Datos/Prácticas/Práctica2/commodity_trade_statistics_data_2006.csv", head=TRUE, sep = ",")

```

- - -

##3.- Limpieza de los datos

Obtenemos un resumen de los datos para llevar a cabo un análisis previo:
```{r, include=TRUE, echo=TRUE, results='hold'}
#resumen de los datos
summary(cmm_trade_data)
```

Llevamos a cabo un análisis "manual" de la información. Vemos que para el campo "comm_code" tenemos dos valores especiales: "999999", que significa "Commodities not specified according to kind" y "TOTAL", que representa, por país y año, el total en USD de cada uno de los posibles flujos (exportaciones, importaciones, re-exportaciones y re-importaciones).

En cuanto a la variable "category", vemos que se trata de una cadena que concatena dos dígitos y un texto; para su tratamiento, separaremos los dos dígitos del resto. Hemos visto un valor especial, se trata de "all_commodities", se trata del valor que se carga para "comm_code=TOTAL", por tanto, para estas observaciones cargaremos un NA.



A continuación, mostramos diagramas de caja para los valores de cada una de las columnas del archivo:
```{r, include=TRUE, echo=FALSE, results='hold'}
##Gr?ficos
# boxplot(whr_data[, c("HS", "LCI", "UCI")], col="grey", xlab='', ylab='', horizontal=TRUE)
# boxplot(whr_data[, c("GpC", "Family", "LE", "DR")], col="grey", xlab='', ylab='', horizontal=TRUE)
# boxplot(whr_data[, c("Freedom", "GC", "Generosity")], col="grey", xlab='', ylab='', horizontal=TRUE)
```


blah blah


```{r, include=TRUE, echo=FALSE, results='hold'}
## Creamos un dataframe con la información sobre código de categoría - descripción

# categories = cmm_trade_data[c("category")]
# categories = subset(categories, category != "all_commodities" | category == "NA")
# categories = categories[!duplicated(categories$category), ]
# categories = data.frame(categories)
# categories = data.frame("category"=categories)
# colnames(categories)[which(names(categories) == "categories")] <- "category"

# de esta forma crearíamos una tabla con frecuencias
# categories <- as.data.frame(ftable(categories$category))

# separamos el código de la descripción
# categories$category_number <- substring(categories$category, 1, 2)
# categories$category_desc <- substring(categories$category, 4)
# almacenamos el código como numérico
# categories$category_number <- as.numeric(unlist(categories$category_number))

## Actualizamos los valores de "category" del dataset de origen
# para las filas con category=all_commodities, dejamos un NA
# cmm_trade_data[which(cmm_trade_data$category=='all_commodities'),]$category <- "NA"
# sustituimos el valor por la clave num?rica
# cmm_trade_data$category = substring(cmm_trade_data$category, 1, 2)
# cmm_trade_data$category <- as.numeric(unlist(cmm_trade_data$category))

```



###3.1.- ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos?



###3.2.- Identificación y tratamiento de valores extremos.

- - -

##4.- Análisis de los datos.

###4.1.- Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).

###4.2.- Comprobación de la normalidad y homogeneidad de la varianza.

###4.3.- Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc.

- - -

##5.- Representación de los resultados a partir de tablas y gráficas.

- - -

##6.- Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?

- - -

##7.- Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar en Python.

- - -

##Criterios de valoración

Todos los apartados son obligatorios. La ponderación de los ejercicios es la siguiente:

* Los apartados 1, 2 y 6 valen 0,5 puntos.
* Los apartados 3, 5 y 7 valen 2 puntos.
* El apartado 4 vale 2,5 puntos.

Se valorará la idoneidad de las respuestas, que deberán ser claras y completas. Las
diferentes etapas deberán justificarse y acompañarse del código correspondiente.

También se valorará la síntesis y claridad, a través del uso de comentarios, del código
resultante, así como la calidad de los datos finales analizados.

- - -
- - -
