---
title: "Práctica 2 - Limpieza y validación de datos"
author: "Rubén Salamanqués y Ricardo Pardo"
date: "10 de junio de 2018"
output:
  html_document:
    toc: true
    #theme: united
    df_print: paged
    
#csl: apa.csl
#bibliography: bibliography.bib    
---
- - -

<style>
body {text-align: justify}
</style>

```{r setup, include=FALSE}
library('knitr')
library('kableExtra')
library('stringr')

knitr::opts_chunk$set(echo = TRUE)
```

##Introducción

El objetivo de esta actividad será el tratamiento de un dataset, que puede ser el creado en la práctica 1 o bien cualquier dataset libre disponible en Kaggle (https://www.kaggle.com). Algunos ejemplos de dataset con los que podéis trabajar son:

* Red Wine Quality (https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)
* Titanic: Machine Learning from Disaster (https://www.kaggle.com/c/titanic)
* Predict Future Sales (https://www.kaggle.com/c/competitive-data-sciencepredict-future-sales/).

Los últimos dos ejemplos corresponden a competiciones activas de Kaggle de manera que, opcionalmente, podríais aprovechar el trabajo realizado durante la práctica para entrar en alguna de estas competiciones.

Para llevar a cabo el trabajo de esta práctica, hemos escogido el dataset "Global Commodity Trade Statistics", que se encuentra disponible en: https://www.kaggle.com/unitednations/global-commodity-trade-statistics/data


A continuación, siguiendo las principales etapas de un proyecto analítico, las diferentes tareas a realizar (y justificar) son las siguientes:


- - -

##1.- Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?

Se trata de un dataset originalmente publicado por "United Nations Statistics Division" en la página de UNData (http://data.un.org/Explorer.aspx). Los términos de uso dicen que todos los datos y metadatos provenientes de la página de UNData pueden ser utilizados sin coste y pueden ser copiados libremente y distribuidos posteriormente siempre y cuando se cite a UNdata como fuente original.

El dataset contiene información sobre flujos de exportación e importación de animales y productos de consumo de países del mundo durante un periodo de 30 años.

Debido al gran volumen que esto supone se va a trabajar con un subconjunto que comprende el periodo entre los años 2006 y 2016.

A partir de estos datos se pueden llevar a cabo estudios sobre los flujos de exportaciones e importaciones de los países en un periodo en el que se ha producido una de las mayores crisis a nivel mundial del último siglo. Las preguntas a las que se pretende dar respuesta son las siguientes:

  * ¿Se ha reducido el volumen comercial (en $) en este periodo para los países del G20?

  * ¿Cuál será la previsión de exportación de la categoría más exportada por España?

  * ¿Las exportaciones de vegetales italianas son más abundantes que las españolas?

Las columnas del dataset son las siguientes:

  * **country_or_area:** País o zona geográfica.
  * **year:** Año de la transacción.
  * **comm_code:** Código de la mercancía.
  * **commodity:** Descripción de la mercancía.
  * **flow:** Flujo de la transacción (importación/exportación/re-importación/re-exportación).
  * **trade_usd:** Precio de la transacción (en dólares estadounidenses).
  * **weight_kg:** Peso en kilogramos.
  * **quantity_name:** Magnitud de la cantidad.
  * **quantity:** Cantidad
  * **category:** Categoría genérica de la transacción

- - -

##2.- Integración y selección de los datos de interés a analizar

Como el archivo original es muy grande, se ha preparado una versión que contiene únicamente los datos a partir del año 2006. Con el siguiente código se carga dicho archivo.

```{r, include=TRUE, echo=FALSE, results='hold'}

vpath_rsb1 = "C:/Users/Ruben/Google Drive/UOC/2_Tipología y Ciclo de Vida de los Datos/Prácticas/Práctica2/commodity_trade_statistics_data_2006.csv"

vpath_rsb2 = "C:/Users/Rsb/Google Drive/UOC/2_Tipología y Ciclo de Vida de los Datos/Prácticas/Práctica2/commodity_trade_statistics_data_2006.csv"

vpath_rpc1 = "C:/Users/Ricky/Google Drive/Máster Data Science/2017-2018 2ndo semestre/Tipología y ciclo de vida de los datos/Práctica 2 - Limpieza y validación de datos/commodity_trade_statistics_data_2006.csv"

vpath_rpc2 = "C:/Users/rpc/Documents/Google Drive/Máster Data Science/2017-2018 2ndo semestre/Tipología y ciclo de vida de los datos/Práctica 2 - Limpieza y validación de datos/commodity_trade_statistics_data_2006.csv"

vpath = vpath_rpc1
```

```{r, results='hold'}
# lectura del fichero con datos a partir de 2006
cmm_trade_data = read.csv(vpath, head=TRUE, sep = ",")
```

```{r, include=TRUE, echo=FALSE, results='hold'}
#borrado de variables
rm(vpath, vpath_rsb1, vpath_rsb2, vpath_rpc1, vpath_rpc2)
```

Con esto se obtiene un conjunto de datos con las características explicadas anteriormente. Sin embargo, para responder a las cuestiones planteadas, tan solo es necesario trabajar con las siguientes columnas.

  * **country_or_area:** Para hacer análisis por países.
  * **year:** Para estudiar la evolución temporal.
  * **comm_code:** Para corregir datos ausentes en otros campos.
  * **flow:** Para estudiar solo ciertos flujos.
  * **trade_usd:** Para analizar los volúmenes de las transacciones.
  * **weight_kg:** Para incluir en modelos de regresión.
  * **category:** Para agrupar por categorías.
  
Además, se va a trabajar únicamente con los países del G-20 (incluyendo a España en lugar de la UE en el último puesto).

El siguiente código en R proporciona un subconjunto de los datos cargados que cumple los requisitos expuestos.

```{r, results='hold'}
G_20_data <- as.data.frame(cmm_trade_data[grep("Argentina|Australia|Brazil|Canada|China|France|Germany|India|Indonesia|Italy|Japan|Mexico|Russia|Saudi Arabia|South Africa|Spain|Rep. of Korea|Turkey|United Kingdom|USA", cmm_trade_data$country_or_area),
                    c("country_or_area",
                      "year",
                      "comm_code",
                      "flow",
                      "trade_usd",
                      "weight_kg",
                      "category")])

rm(cmm_trade_data)
```

- - -

##3.- Limpieza de los datos

Obtenemos un resumen de los datos para llevar a cabo un análisis previo:

```{r, include=TRUE, echo=TRUE, results='hold'}
#resumen de los datos
summary(G_20_data)

# str(G_20_data)

# tipos de las variables:
cat("\n")
cat("Tipo de dato de cada variable:\n")
sapply(G_20_data, function(x) class(x))
```

###3.1.- ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos?

Según hemos definido anteriormente, las columnas con las que vamos a trabajar son las que enumeramos a continuación, se trata de todas las columnas del dataset "G_20_data":
  
  * **country_or_area:** Para hacer análisis por países.
  * **year:** Para estudiar la evolución temporal.
  * **comm_code:** Para corregir datos ausentes en otros campos.
  * **flow:** Para estudiar solo ciertos flujos.
  * **trade_usd:** Para analizar los volúmenes de las transacciones.
  * **weight_kg:** Para analizar los volúmenes, en peso, de las transacciones.
  * **category:** Para agrupar por categorías.

Por tanto, estudiaremos la existencia de ceros o elementos vacíos en estas columnas:

```{r, include=TRUE, echo=TRUE, results='hold'}

# Números de valores desconocidos por campo
sapply(G_20_data, function(x) sum(is.na(x) | x==0))

```

Vemos que la única columna que contiene ceros y/o 'NA' es "weight_kg", por tanto, tenemos dos opciones, la primera sería eliminar estos registros, la segunda sería tratar esta columna para asignar un valor válido para nuestros análisis, escogeremos esta segunda opción dado que la primera nos llevaría a rechazar registros válidos para el proyecto.

Para llevar a cabo esta asignación valores, realizaremos una imputación de valores por media simple:

```{r, include=TRUE, echo=TRUE, results='hold', message=FALSE}

G_20_data = transform(G_20_data, weight_kg = ifelse((is.na(weight_kg)|weight_kg==0), mean(weight_kg, na.rm=TRUE), weight_kg))

sapply(G_20_data, function(x) sum(is.na(x) | x==0))

```


Por otro lado, vamos analizar en detalle las variables **comm_code** y **category** ya que hemos visto que contienen valores especiales:
 
```{r, include=TRUE, echo=TRUE, results='hold'}
## Analizamos la columna comm_code
print("Valores de comm_code:")
summary(G_20_data$comm_code)
```

Los valores especiales que encontramos son los siguientes:    

  + "999999": que significa "Commodities not specified according to kind"
  + "TOTAL": representa, por país y año, el total en USD de cada uno de los posibles flujos (exportaciones, importaciones, re-exportaciones y re-importaciones).
  
Tendremos en cuenta estos valores en los puntos posteriores ya que no hemos de tratarlos como al resto de observaciones.
 
```{r, include=TRUE, echo=TRUE, results='hold'}
## Analizamos la columna category
print("Valores de category:")
summary(G_20_data$category)
```

Vemos que para el caso de la variable "category", existe un valor especial, se trata de "all_commodities", este valor se corresponde con observaciones en las que "comm_code=TOTAL", por tanto, trataremos estos casos de forma especial.

###3.2.- Identificación y tratamiento de valores extremos

Dentro del conjunto de datos que se está utilizando, los únicos datos sobre los que tiene sentido realizar un estudio sobre valores extremos son: *trade_usd* y *weight_kg*.

Como se acaba de ver en el punto anterior, el conjunto de datos cuenta con filas sumatorio (comm_code= TOTAL o 999999). Estas filas no son válidas para realizar los análisis propuestos con lo que se procede a limpiar el conjunto de datos eliminándolas.

```{r, include=TRUE, results='hold'}

G_20_data <- G_20_data[G_20_data$comm_code != "999999" &
                       G_20_data$comm_code != "TOTAL",]

```

En primer lugar, analizaremos los valores extremos para la variable *trade_usd*. Una manera de visualizar los valores extremos es mediante un diagrama de caja o *boxplot*. Con el siguiente código se obtiene este diagrama.

```{r, include=TRUE, results='hold'}

par(mar=c(7,4,7,4)) #Márgenes del gráfico

boxplot(G_20_data$trade_usd, 
       main="Dispersión de trade_usd",
       xlab="Dólares estadounidenses",
       col="skyblue", #color de relleno
       boxcol="skyblue", #color de la caja
       outcol="salmon", #color de los valores extremos
       horizontal=TRUE)

```

Se puede observar que el gráfico obtenido no es legible. Esto se debe a que el campo analizado tiene un rango de valores muy grande. En el conjunto de datos se registran transacciones de varios cientos de miles de millones de dólares así como otras de tan solo decenas.

En realidad, cualquiera de estos valores es un valor válido con lo que no se considera ningún registro como valor extremo.

Por último se muestra la distribución de valores de *trade_usd* para apreciar la cantidad de datos que existen a lo largo de todo el rango de la variable.

```{r, include=TRUE, results='hold'}

#cat("Valores atípicos para trade_usd:", boxplot.stats(G_20_data[,"trade_usd"])$out)
plot(G_20_data$trade_usd, 
     xlab="Valores de trade_usd",
     ylab="Fila",
     col="skyblue"
     )

```


A continuación, analizaremos la variable *weight_kg*. El primer paso será mostrar el diagrama de cajas:

```{r, include=TRUE, results='hold'}

par(mar=c(7,4,7,4)) #Márgenes del gráfico

boxplot(G_20_data$weight_kg, 
       main="Dispersión de weight_kg",
       xlab="Kg",
       col="skyblue", #color de relleno
       boxcol="skyblue", #color de la caja
       outcol="salmon", #color de los valores extremos
       horizontal=TRUE)

```

En este caso obtenemos un gráfico similar al anterior, vemos que la variable *weight_kg* tiene un rango de valores muy grande. En este caso tomaremos también como válidos todos los valores. 

Se muestra a continuación la distribución de valores de *weight_kg*:


```{r, include=TRUE, results='hold'}

plot(G_20_data$weight_kg, 
     xlab="Valores de weight_kg",
     ylab="Fila",
     col="skyblue"
     )

```


- - -

##4.- Análisis de los datos

En este punto se llevará a cabo un segundo análisis de los datos para conocerlos con un mayor grado de detalle antes de comenzar con el análisis final, el cual nos llevará a la resolución de las preguntas planteadas en el punto 1.

###4.1.- Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar)

En el punto 2, tras un análisis preliminar, hemos definido el conjunto de datos de trabajo mediante una preselección del conjunto original, seleccionando únicamente las variables del conjunto original que hemos estimado que iban a ser necesarias. A partir de dicho conjunto de datos, "G_20_data", vamos a analizar qué variables serán necesarias para dar respuesta a cada una de las preguntas planteadas y, a partir de ello, definiremos un dataset para dar respuesta a cada una de dichas preguntas:

#### 4.1.1.- ¿Se ha reducido el volumen comercial (en $) en este periodo para los países del G20?

Para la resolución de la primera pregunta planteada: "¿Se ha reducido el volumen comercial (en $) en este periodo para los países del G20?", será necesario obtener un conjunto de datos que agrupe por país y por año el volumen de transacciones totales. Para ello se hará uso de la bilbioteca *dplyr* de R.

```{r, include=TRUE, echo=TRUE, warning=FALSE}

library(dplyr, warn.conflicts=FALSE) #Para manipular los datos

```

Ahora se puede generar el conjunto de datos, se cambian las unidades del sumatorio a miles de millones de dólares para reducir la cifra resultante.

```{r, include=TRUE, results='hold', fig.width=8,fig.height=10}

#Sumatorio por país y año
country_year_data <- G_20_data %>% 
                    group_by(year,country_or_area) %>%
                    summarise(suma = sum(trade_usd))

#Se cambia la unidad del sumatorio, de dólares a miles de millones de dólares
country_year_data$suma <- country_year_data$suma/1000000000

```

#### 4.1.2.- ¿Cuál será la previsión de exportación de la categoría más exportada por España?
    
Preparamos los datos que vamos a necesitar para este análisis. Tomaremos las exportaciones totales de España, agrupando por categorías.

```{r, include=TRUE, results='hold'}

exportaciones_ES <- G_20_data[G_20_data$country_or_area == "Spain" & 
                              G_20_data$flow == "Export",] %>% 
                    group_by(category) %>%
                    summarise(suma = sum(trade_usd)) 

```

#### 4.1.3.- ¿Las exportaciones de vegetales italianas son más abundantes que las españolas?

Para resolver la tercera cuestión, utilizaremos los siguientes conjuntos de datos:

```{r, include=TRUE, results='hold'}

#Datos de Italia
veg_IT <- G_20_data[G_20_data$country_or_area == "Italy" & 
                    G_20_data$flow == "Export" &
                    G_20_data$category == 
                    "19_cereal_flour_starch_milk_preparations_and_products",]$trade_usd

#Datos de España
veg_ES <- G_20_data[G_20_data$country_or_area == "Spain" & 
                    G_20_data$flow == "Export" &
                    G_20_data$category == 
                    "19_cereal_flour_starch_milk_preparations_and_products",]$trade_usd
```


###4.2.- Comprobación de la normalidad y homogeneidad de la varianza

A continuación, procederemos a analizar la normalidad y hogeneidad de la varianza de los valores que utilizaremos en el estudio.

#### 4.2.1.- Comprobación de normalidad

Necesitamos comprobar si los valores que toman las variables cuantitativas se ajustan a una distribución normal, para ello podemos utilizar alguno de los test disponibles en las librerías "normtest", "nortest" y "moments". En nuestro caso, utilizaremos la prueba de normalidad de **Anderson-Darling**, disponible en la librería "nortest".

```{r, include=TRUE, results='hold'}
sapply(G_20_data, function(x) class(x))
```


Estableceremos un nivel de significación $\alpha = 0.05$, una vez aplicado el test, si obtenemos un $p-valor$ superior a dicho nivel de significación, se considerará que la variable sigue una distribución normal.

```{r, include=TRUE, results='hold'}
# para llevar a cabo este test, utilizaremos la librería normtest:
library(nortest)

alpha = 0.05

print("Análisis de normalidad de variables numéricas:")

for (i in 1:ncol(G_20_data)) {
  
    # si la columna es numérica, aplicamos el estudio
    if (is.integer(G_20_data[,i]) | is.numeric(G_20_data[,i])){
      
      # llevamos a cabo el test de Anderson-Darling
      p_valor = ad.test(G_20_data[,i])$p.value
      
      if (p_valor > alpha){
        cat("La variable", colnames(G_20_data)[i], "sí sigue una distribución normal")
      }else{
        cat("La variable", colnames(G_20_data)[i], "no sigue una distribución normal")
      }
      cat("\n")
    }
  }

```

#### 4.2.2.- Homogeneidad de la varianza

A continuación, estudiaremos la homogeneidad de varianzas en cuanto a los diferentes países, para ello aplicaremos el test de **Fligner-Killen**. La hipótesis nula representa que las varianzas son iguales.

Para el estudio de la homogeneidad de la varianza, estableceremos el mismo nivel de significación que para el estudio de la normalidad: $\alpha = 0.05$

```{r, include=TRUE, results='hold'}

flt <- fligner.test(trade_usd ~ country_or_area, data=G_20_data)

print(flt)

p_valor <- flt$p.value

if (p_valor > alpha){
  print("Las muestras sí son homogéneas")
}else{
  print("Las muestras no son homogéneas")
}

```

```{r, include=TRUE, echo=FALSE, results='hold'}
#borrado de variables
rm(alpha, i, flt, p_valor)
```

###4.3.- Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc.

####4.3.1.- ¿Se ha reducido el volumen comercial (en $) en este periodo para los países del G20?

Para responder a esta pregunta tan solo será necesario realizar un gráfico con la evolución por años de las cifras de importaciones o exportaciones de cada país.

Para ello es necesario cargar la siguiente biblioteca de R:

```{r, include=TRUE, results='hold', warning=FALSE}

library(ggplot2) #Para crear el gráfico

```

Haciendo uso del conjunto de datos **country_year_data** obtenido en el punto 4.1, se pinta el gráfico deseado.

```{r, include=TRUE, results='hold', fig.width=8,fig.height=10}

#Gráfico
ggplot(country_year_data, aes(x=year, y=suma, shape=country_or_area, color=country_or_area)) +
  scale_shape_manual(values=seq(0,21))+ #Símbolos para cada país
  geom_line()+ #Se pinta la línea entre los puntos
  geom_point()+ #Se pintan los puntos
  labs(title="Evolución anual de transacciones del G20",
       x="Año",
       y="Miles de Millones de Dólares")+
  theme(legend.position="top")+ 
  theme(legend.text = element_text(size=8))+
  theme(legend.title = element_blank())

```

Se puede observar que en líneas generales el volumen comercial sufrió una caída fuerte en el año 2009 pero, a 2016, los países del G20 tienen el mismo volumen (sino más) que en 2006.


####4.3.2.- ¿Cuál será la previsión de exportación de la categoría más exportada por España?

Para responder a esta pregunta se creará un modelo de regresión lineal capaz de hacer predicciones sobre las exportaciones de España.

Lo primero que es necesario obtener es la categoría más exportada por España. Se va a considerar esa categoría como aquella que haya movido más volumen monetario dentro del periodo estudiado.

Con el conjunto de datos obtenido en el punto 4.1.2 se muestra qué categoría es la más exportada.


```{r, include=TRUE, echo=TRUE}

#Máxima categoría
print(exportaciones_ES[which.max(exportaciones_ES$suma),])

``` 

Se obtiene que la categoría con más volumen de exportaciones es *88_aircraft_spacecraft_and_parts_thereof*. Se van a generar varios modelos de regresión lineal a fin de compararlos y realizar una predicción con el mejor de ellos.

Para que los modelos sean lo más precisos posible, los datos con los que se van a entrenar van a estar limitados a la categoría obtenida y a España.

```{r, include=TRUE, echo=TRUE}

modelo1 <- lm(trade_usd ~ year + comm_code, 
              data = 
                  G_20_data[G_20_data$category == "88_aircraft_spacecraft_and_parts_thereof" &
                            G_20_data$country_or_area == "Spain"
                            ,])

modelo2 <- lm(trade_usd ~ year + weight_kg + comm_code,
              data = 
                  G_20_data[G_20_data$category == "88_aircraft_spacecraft_and_parts_thereof" &
                            G_20_data$country_or_area == "Spain"
                            ,])

modelo3 <- lm(trade_usd ~ year + flow + weight_kg + comm_code,
              data = 
                  G_20_data[G_20_data$category == "88_aircraft_spacecraft_and_parts_thereof" &
                            G_20_data$country_or_area == "Spain"
                            ,])

```

Tras generar los modelos se comparan gracias a su coeficiente de determinación $R^2$. Cuánto más alto sea su valor mejor explicará ese modelo la variabilidad en el precio de la transacción.

```{r, include=TRUE, echo=TRUE}

coeficientes <- matrix(c(1, summary(modelo1)$r.squared,
                         2, summary(modelo2)$r.squared,
                         3, summary(modelo3)$r.squared),
                       ncol = 2, byrow = TRUE)

colnames(coeficientes) <- c("Modelo", "R^2")

coeficientes

```

Se observa que el mejor modelo es el que predice el precio gracias al año, el tipo de flujo, el peso y el código de la materia. Con un valor de 0.7864493 tenemos que el modelo puede explicar casi el 79% de la variabilidad del precio. Aunque es una cifra mejorable, es lo suficientemente alta como para poder hacer estimaciones.

Ahora se pueden predecir exportaciones futuras para España en materias que sean de la categoría estudiada. Por ejemplo, se predice por cúanto se podrá exportar 25.000 Kg de partes de aviones (código de material 880330) en 2020.

```{r}

datos_pred <- data.frame(year=2020, flow = "Export", weight_kg = 25000, comm_code = "880330")

cat("Precio de exportación previsto:", predict.lm(modelo3, datos_pred))

```

####4.3.3.- ¿Las exportaciones de vegetales italianas son más abundantes que las españolas?

Para responder a esta pregunta se va a realizar un contraste de hipótesis de dos muestras sobre la diferencia de medias, las exportaciones de vegetales de italia y las de españa.

Este tipo de contrastes requiere que los datos empleados sigan una distribución normal, en principio nuestro conjunto de datos no cumple con esta característica pero, el contraste que vamos a realizar es un contraste sobre la diferencia de medias y, según el teorema del límite central, las medias de un conjunto de datos pueden aproximarse a la normal si la muestra es suficientemente grande (más de 30). En nuestro caso esto se cumple con creces con lo que se puede aplicar el contraste sin problemas. 

La hipótesis es la siguiente:

  * Hipótesis nula H~0~: $\mu_1 - \mu_2 = 0$

  * Hipótesis alternativa H~1~: $\mu_1 - \mu_2 > 0$

Siendo $\mu_1$ la media de las exportaciones de vegetales italianas y $\mu_2$ la media de las españolas En este caso se tiene una hipótesis unilateral. Se tomará un nivel de significación $\alpha$ = 0.05.

Con los conjuntos de datos obtenidos en el punto 4.1.3 se realiza el contraste de hipótesis.

```{r, include=TRUE, results='hold'}

#Contraste de hipótesis
t.test(veg_IT, veg_ES, alternative = "greater")

```

Se obtiene un p-valor de 1.993e-07, muy inferior al nivel de significación $\alpha$ fijado con lo que se rechaza la hipótesis nula y se concluye que las exportaciones italianas en la categoría *19_cereal_flour_starch_milk_preparations_and_products* son más abundates que las españolas en esa misma categoría.

```{r, include=TRUE, echo=FALSE, results='hold'}
#borrado de variables
rm(modelo1, modelo2, modelo3, datos_pred, coeficientes, veg_ES, veg_IT)
```

- - -

##5.- Representación de los resultados a partir de tablas y gráficas

En los puntos dónde eran necesarios, se muestran gráficas y tablas que ayudan a interpretar los resultados o apoyan las explicaciones.

- - -

##6.- Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?

En el punto 4.3 se han aplicado las pruebas necesarias para dar respuesta a cada una de la preguntas planteadas, utilizando diferentes métodos para cada una de ellas: análisis gráfico, regresión lineal y contraste de hipótesis. Cada una de las preguntas ha obtenido una respuesta adecuada, por lo que se puede decir que el análisis ha concluido de forma satisfactoria.

- - -

##7.- Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar en Python.

El propio documento incluye el código que se ha utilizado para la realización del trabajo.

- - -

##Criterios de valoración

Todos los apartados son obligatorios. La ponderación de los ejercicios es la siguiente:

* Los apartados 1, 2 y 6 valen 0,5 puntos.
* Los apartados 3, 5 y 7 valen 2 puntos.
* El apartado 4 vale 2,5 puntos.

Se valorará la idoneidad de las respuestas, que deberán ser claras y completas. Las
diferentes etapas deberán justificarse y acompañarse del código correspondiente.

También se valorará la síntesis y claridad, a través del uso de comentarios, del código
resultante, así como la calidad de los datos finales analizados.

- - -
- - -