---
title: "Práctica 2 - Limpieza y validación de datos"
author: "Rubén Salamanqués y Ricardo Pardo"
date: "10 de junio de 2018"
output:
  html_document:
    toc: true
    #theme: united
    df_print: paged
    
#csl: apa.csl
#bibliography: bibliography.bib    
---
- - -

<style>
body {text-align: justify}
</style>

```{r setup, include=FALSE}
library('knitr')
library('kableExtra')
library('stringr')

knitr::opts_chunk$set(echo = TRUE)
```

##Introducción

El objetivo de esta actividad será el tratamiento de un dataset, que puede ser el creado en la práctica 1 o bien cualquier dataset libre disponible en Kaggle (https://www.kaggle.com). Algunos ejemplos de dataset con los que podéis trabajar son:

* Red Wine Quality (https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)
* Titanic: Machine Learning from Disaster (https://www.kaggle.com/c/titanic)
* Predict Future Sales (https://www.kaggle.com/c/competitive-data-sciencepredict-future-sales/).

Los últimos dos ejemplos corresponden a competiciones activas de Kaggle de manera que, opcionalmente, podríais aprovechar el trabajo realizado durante la práctica para entrar en alguna de estas competiciones.

Para llevar a cabo el trabajo de esta práctica, hemos escogido el dataset "Global Commodity Trade Statistics", que se encuentra disponible en: https://www.kaggle.com/unitednations/global-commodity-trade-statistics/data


A continuación, siguiendo las principales etapas de un proyecto analítico, las diferentes tareas a realizar (y justificar) son las siguientes:


- - -

##1.- Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?

Se trata de un dataset originalmente publicado por "United Nations Statistics Division" en la página de UNData (http://data.un.org/Explorer.aspx). Los términos de uso dicen que todos los datos y metadatos provenientes de la página de UNData pueden ser utilizados sin coste y pueden ser copiados libremente y distribuidos posteriormente siempre y cuando se cite a UNdata como fuente original.

El dataset contiene información sobre flujos de exportación e importación de animales y productos de consumo de países del mundo durante un periodo de 30 años.

Debido al gran volumen que esto supone se va a trabajar con un subconjunto que comprende el periodo entre los años 2006 y 2016.

A partir de estos datos se pueden llevar a cabo estudios sobre los flujos de exportaciones e importaciones de los países en un periodo en el que se ha producido una de las mayores crisis a nivel mundial del último siglo.

  * ¿Se ha reducido el volumen comercial (en $) en este periodo para los países del G20?

  * ¿Cuál será la previsión de exportación de la categoría más exportada por España?

  * ¿Las exportaciones de vegetales italianas son más abundantes que las españolas?

Las columnas del dataset son las siguientes:

  * **country_or_area:** País o zona geográfica.
  * **year:** Año de la transacción.
  * **comm_code:** Código de la mercancía.
  * **commodity:** Descripción de la mercancía.
  * **flow:** Flujo de la transacción (importación/exportación/re-importación/re-exportación).
  * **trade_usd:** Precio de la transacción (en dólares estadounidenses).
  * **weight_kg:** Peso en kilogramos.
  * **quantity_name:** Magnitud de la cantidad.
  * **quantity:** Cantidad
  * **category:** Categoría genérica de la transacción

- - -

##2.- Integración y selección de los datos de interés a analizar.

Como el archivo original es muy grande, se ha preparado una versión que contiene únicamente los datos a partir del año 2006. Con el siguiente código se carga dicho archivo.

```{r, results='hold'}

# lectura del fichero con datos a partir de 2006
#cmm_trade_data = read.csv("C:/Users/Ruben/Google Drive/UOC/2_Tipología y Ciclo de Vida de los Datos/Prácticas/Práctica2/commodity_trade_statistics_data_2006.csv", head=TRUE, sep = ",")

cmm_trade_data = read.csv("C:/Users/Rsb/Google Drive/UOC/2_Tipología y Ciclo de Vida de los Datos/Prácticas/Práctica2/commodity_trade_statistics_data_2006.csv", head=TRUE, sep = ",")

#cmm_trade_data = read.csv("C:/Users/rpc/Documents/Google Drive/Máster Data Science/2017-2018 2ndo semestre/Tipología y ciclo de vida de los datos/Práctica 2 - Limpieza y validación de datos/commodity_trade_statistics_data_2006.csv", head=TRUE, sep = ",")

```

Con esto se obtiene un conjunto de datos con las características explicadas anteriormente. Sin embargo, para responder a las cuestiones planteadas, tan solo es necesario trabajar con las siguientes columnas.


  * **country_or_area:** Para hacer análisis por países.
  * **year:** Para estudiar la evolución temporal.
  * **comm_code:** Para corregir datos ausentes en otros campos.
  * **flow:** Para estudiar solo ciertos flujos.
  * **trade_usd:** Para analizar los volúmenes de las transacciones.
  * **weight_kg:** Para incluir en modelos de regresión.
  * **category:** Para agrupar por categorías.
  
Además, se va a trabajar únicamente con los países del G-20 (incluyendo a España en lugar de la UE en el último puesto).

El siguiente código en R proporciona un subconjunto de los datos cargados que cumple los requisitos expuestos.

```{r, results='hold'}
G_20_data <- as.data.frame(cmm_trade_data[grep("Argentina|Australia|Brazil|Canada|China|France|Germany|India|Indonesia|Italy|Japan|Mexico|Russia|Saudi Arabia|South Africa|Spain|Rep. of Korea|Turkey|United Kingdom|USA", cmm_trade_data$country_or_area),
                    c("country_or_area",
                      "year",
                      "comm_code",
                      "flow",
                      "trade_usd",
                      "weight_kg",
                      "category")])
```

- - -

##3.- Limpieza de los datos

Obtenemos un resumen de los datos para llevar a cabo un análisis previo:
```{r, include=TRUE, echo=TRUE, results='hold'}
#resumen de los datos
summary(G_20_data)

str(G_20_data)
```

###3.1.- ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos?

Según hemos definido anteriormente, las columnas con las que vamos a trabajar son las siguientes:
  
  * **country_or_area:** Para hacer análisis por países.
  * **year:** Para estudiar la evolución temporal.
  * **comm_code:** Para corregir datos ausentes en otros campos.
  * **flow:** Para estudiar solo ciertos flujos.
  * **trade_usd:** Para analizar los volúmenes de las transacciones.
  * **weight_kg:** Para incluir en modelos de regresión.
  * **category:** Para agrupar por categorías.

Por tanto, analizaremos la existencia de ceros o elementos vacíos en estas columnas.

Comenzaremos el análisis detallado por las variables más sencillas:

 + En primer lugar **year**:
 
```{r, include=TRUE, echo=TRUE, results='hold'}
## Analizamos la columna year
aux = G_20_data[c("year")]
aux = aux[!duplicated(aux$year), ]

aux_n0 <- G_20_data[ which(G_20_data$year=='0' | G_20_data$year=='NA'), ]

if (nrow(aux_n0) == 0){
  print("No existen nulos ni ceros")
}else{
  print("Sí existen nulos ni ceros")
}

print("-------------------------------------------------------")
print("Los valores que contiene la columna son los siguientes:")
print(sort(aux,decreasing=FALSE))
```

+ A continuación, **flow**, en este caso ya hemos visto en el resumen que todos los valores son de los cuatro tipos esperados:

```{r, include=TRUE, echo=TRUE, results='hold'}
## Analizamos la columna flow
aux = G_20_data[c("flow")]
aux = aux[!duplicated(aux$flow), ]

aux_n0 <- G_20_data[ which(G_20_data$flow=='0' | G_20_data$flow=='NA'), ]

if (nrow(aux_n0) == 0){
  print("No existen nulos ni ceros")
}else{
  print("Sí existen nulos ni ceros")
}

print("-------------------------------------------------------")
print("Los valores que contiene la columna son los siguientes:")
print(sort(aux,decreasing=FALSE))
```


 + La siguiente variable a analizar es **country_or_area**:
 
```{r, include=TRUE, echo=TRUE, results='hold'}
## Analizamos la columna country_or_area
aux = G_20_data[c("country_or_area")]
aux = aux[!duplicated(aux$country_or_area), ]

aux_n0 <- G_20_data[ which(G_20_data$country_or_area=='0' | G_20_data$country_or_area=='NA'), ]

if (nrow(aux_n0) == 0){
  print("No existen nulos ni ceros")
}else{
  print("Sí existen nulos ni ceros")
}

print("-------------------------------------------------------")
print("Los valores que contiene la columna son los siguientes:")
print(sort(aux,decreasing=FALSE))
```

 + La siguiente variable a analizar es **comm_code**:
 
```{r, include=TRUE, echo=TRUE, results='hold'}
## Analizamos la columna comm_code
aux = G_20_data[c("comm_code")]
aux = aux[!duplicated(aux$comm_code), ]

aux_n0 <- G_20_data[ which(G_20_data$comm_code=='0' | G_20_data$comm_code=='NA'), ]

if (nrow(aux_n0) == 0){
  print("No existen nulos ni ceros")
}else{
  print("Sí existen nulos ni ceros")
}

print("-------------------------------------------------------")
print("Los valores que contiene la columna son los siguientes:")
summary(G_20_data$comm_code)
```

    - Para el caso de "comm_code", vemos que existen dos valores aparentemente anómalos o especiales, los trataremos en el punto 3.2, son los siguientes valores:

      - "999999": que significa "Commodities not specified according to kind"
  
      - "TOTAL": representa, por país y año, el total en USD de cada uno de los posibles flujos (exportaciones, importaciones, re-exportaciones y re-importaciones).


 + La siguiente variable a analizar es **trade_usd**:
 
```{r, include=TRUE, echo=TRUE, results='hold'}
## Analizamos la columna trade_usd
aux = G_20_data[c("trade_usd")]
aux = aux[!duplicated(aux$trade_usd), ]

aux_n0 <- G_20_data[ which(G_20_data$trade_usd=='0' | G_20_data$trade_usd=='NA'), ]

if (nrow(aux_n0) == 0){
  print("No existen nulos ni ceros")
}else{
  print("Sí existen nulos ni ceros")
}

print("-------------------------------------------------------")
print("Los valores que contiene la columna son los siguientes:")
summary(G_20_data$trade_usd)
```

 + La siguiente variable a analizar es **category**:
 
```{r, include=TRUE, echo=TRUE, results='hold'}
## Analizamos la columna category
aux = G_20_data[c("category")]
aux = aux[!duplicated(aux$category), ]

aux_n0 <- G_20_data[ which(G_20_data$category=='0' | G_20_data$category=='NA'), ]

if (nrow(aux_n0) == 0){
  print("No existen nulos ni ceros")
}else{
  print("Sí existen nulos ni ceros")
}

print("-------------------------------------------------------")
print("Los valores que contiene la columna son los siguientes:")
print(sort(aux,decreasing=FALSE))
```

En cuanto a la variable "category", vemos que se trata de una cadena que concatena dos dígitos y un texto. Hemos visto un valor especial, se trata de "all_commodities", se trata del valor que se carga para "comm_code=TOTAL", por tanto, trataremos estas observciones de forma especial.


###3.2.- Identificación y tratamiento de valores extremos.

Dentro del conjunto de datos que se está utilizando, el único dato sobre el que tiene sentido realizar un estudio sobre valores extremos es *trade_usd*.

Como se acaba de ver en el punto anterior, el conjunto de datos cuenta con filas sumatorio (comm_code= TOTAL o 999999). Estas filas no son válidas para realizar los análisis propuestos con lo que se procede a limpiar el conjunto de datos eliminándolas.

```{r, include=TRUE, results='hold'}

G_20_data <- G_20_data[G_20_data$comm_code != "999999" &
                       G_20_data$comm_code != "TOTAL",]

```

Una manera de visualizar los valores extremos es mediante un diagrama de caja o *boxplot*. Con el siguiente código se obtiene este diagrama.

```{r, include=TRUE, results='hold'}

par(mar=c(7,4,7,4)) #Márgenes del gráfico

boxplot(G_20_data$trade_usd, 
       main="Dispersión de trade_usd",
       xlab="Dólares estadounidenses",
       col="skyblue", #color de relleno
       boxcol="skyblue", #color de la caja
       outcol="salmon", #color de los valores extremos
       horizontal=TRUE)

```

Se puede observar que el gráfico obtenido no es legible. Esto se debe a que el campo analizado tiene un rango de valores muy grande. En el conjunto de datos se registran transacciones de varios cientos de miles de millones de dólares así como otras de tan solo decenas.

En realidad, cualquiera de estos valores es un valor válido con lo que no se considera ningún registro como valor extremo.

Por último se muestra la distribución de valores de *trade_usd* para apreciar la cantidad de datos que exiten a lo largo de todo el rango de la variable.

```{r, include=TRUE, results='hold'}

#cat("Valores atípicos para trade_usd:", boxplot.stats(G_20_data[,"trade_usd"])$out)
plot(G_20_data$trade_usd, 
     xlab="Valores de trade_usd",
     ylab="Fila",
     col="skyblue"
     )

```

- - -

##4.- Análisis de los datos.

###4.1.- Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).


Para la resolución de la primera pregunta planteada será necesario obtener un conjunto de datos que agrupe por país y por año el volumen de transacciones totales. Para ello se hará uso de la bilbioteca *dplyr* de R.

```{r, include=TRUE, echo=FALSE, warning=FALSE}

library(dplyr) #Para manipular los datos

```

Ahora se puede generar el conjunto de datos, se cambian las unidades del sumatorio a miles de millones de dólares para reducir la cifra resultante.

```{r, include=TRUE, results='hold', fig.width=8,fig.height=10}

#Sumatorio por país y año
country_year_data <- G_20_data %>% 
                    group_by(year,country_or_area) %>%
                    summarise(suma = sum(trade_usd))

#Se cambia la unidad del sumatorio, de dólares a miles de millones de dólares
country_year_data$suma <- country_year_data$suma/1000000000

```

###4.2.- Comprobación de la normalidad y homogeneidad de la varianza.

```{r, include=TRUE, results='hold'}
library(normtest)
```

```{r, include=TRUE, results='hold'}

plot(density(G_20_data$trade_usd), 
     xlab="Valores de trade_usd",
     ylab="Densidad",
     col="skyblue"
     )

```

```{r, include=TRUE, results='hold'}
#wb.norm.test(G_20_data$trade_usd)

```


###4.3.- Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc.

####4.3.1.- ¿Se ha reducido el volumen comercial (en $) en este periodo para los países del G20?

Para responder a esta pregunta tan solo será necesario realizar un gráfico con la evolución por años de las cifras de importaciones o exportaciones de cada país.

Para ello es necesario cargar la siguiente biblioteca de R:

```{r, include=TRUE, results='hold', warning=FALSE}

library(ggplot2) #Para crear el gráfico

```

Haciendo uso del conjunto de datos **country_year_data** obtenido en el punto 4.1, se pinta el gráfico deseado.

```{r, include=TRUE, results='hold', fig.width=8,fig.height=10}

#Gráfico
ggplot(country_year_data, aes(x=year, y=suma, shape=country_or_area, color=country_or_area)) +
  scale_shape_manual(values=seq(0,21))+ #Símbolos para cada país
  geom_line()+ #Se pinta la línea entre los puntos
  geom_point()+ #Se pintan los puntos
  labs(title="Evolución anual de transacciones del G20",
       x="Año",
       y="Miles de Millones de Dólares")+
  theme(legend.position="top")+ 
  theme(legend.text = element_text(size=8))+
  theme(legend.title = element_blank())


```

####4.3.2.- ¿Cuál será la previsión de exportación de la categoría más exportada por España?

Para responder a esta pregunta se creará un modelo de regresión lineal capaz de hacer predicciones sobre las exportaciones de España.

Lo primero que es necesario obtener es la categoría más exportada por España. Se va a considerar esa categoría como aquella que haya movido más volumen monetario dentro del periodo estudiado.


```{r, include=TRUE, echo=TRUE}

#Sumatorio de exportaciones españolas por categoría
exportaciones_ES <- G_20_data[G_20_data$country_or_area == "Spain" & 
                              G_20_data$flow == "Export",] %>% 
                    group_by(category) %>%
                    summarise(suma = sum(trade_usd))

#Máxima categoría
print(exportaciones_ES[which.max(exportaciones_ES$suma),])

```

Se obtiene que la categoría con más volumen de exportaciones es *88_aircraft_spacecraft_and_parts_thereof*. Se van a generar varios modelos de regresión lineal a fin de compararlos y realizar una predicción con el mejor de ellos.


```{r, include=TRUE, echo=TRUE}

modelo1 <- lm(trade_usd ~ country_or_area + year, 
              data = G_20_data[G_20_data$flow == "Export",])

modelo2 <- lm(trade_usd ~ country_or_area + year + category, 
              data = G_20_data[G_20_data$flow == "Export",])

modelo3 <- lm(trade_usd ~ country_or_area + year + weight_kg + category,
              data = G_20_data[G_20_data$flow == "Export",])

```


```{r, include=TRUE, echo=TRUE}

coeficientes <- matrix(c(1, summary(modelo1)$r.squared,
                         2, summary(modelo2)$r.squared,
                         3, summary(modelo3)$r.squared),
                       ncol = 2, byrow = TRUE)

colnames(coeficientes) <- c("Modelo", "R^2")

coeficientes

```



####4.3.3.- ¿Las exportaciones de vegetales italianas son más abundantes que las españolas?

Para responder a esta pregunta se va a realizar un contraste de hipótesis de dos muestras sobre la diferencia de medias, las exportaciones de vegetales de italia y las de españa.

Este tipo de contrastes requiere que los datos empleados sigan una distribución normal, en principio nuestro conjunto de datos no cumple con esta característica pero, el contraste que vamos a realizar es un contraste sobre la diferencia de medias y, según el teorema del límite central, las medias de un conjunto de datos pueden aproximarse a la normal si la muestra es suficientemente grande (más de 30). En nuestro caso esto se cumple con creces con lo que se puede aplicar el contraste sin problemas. 

La hipótesis es la siguiente:

  * Hipótesis nula H~0~: $\mu_1 - \mu_2 = 0$

  * Hipótesis alternativa H~1~: $\mu_1 - \mu_2 > 0$

Siendo $\mu_1$ la media de las exportaciones de vegetales italianas y $\mu_2$ la media de las españolas En este caso se tiene una hipótesis unilateral. Se tomará un nivel de significación $\alpha$ = 0.05.


Se obtienen los grupos sobre los que realizar el contraste, se toma como vegetales la categoría *19_cereal_flour_starch_milk_preparations_and_products*.

```{r, include=TRUE, results='hold'}

veg_IT <- G_20_data[G_20_data$country_or_area == "Italy" & 
                    G_20_data$flow == "Export" &
                    G_20_data$category == 
                    "19_cereal_flour_starch_milk_preparations_and_products",]$trade_usd

veg_ES <- G_20_data[G_20_data$country_or_area == "Spain" & 
                    G_20_data$flow == "Export" &
                    G_20_data$category == 
                    "19_cereal_flour_starch_milk_preparations_and_products",]$trade_usd

t.test(veg_IT, veg_ES, alternative = "greater")
```

Se obtiene un p-valor de 0.0000001993, muy inferior al nivel de significación $\alpha$ fijado con lo que se rechaza la hipótesis nula y se concluye que las exportaciones italianas en la categoría *19_cereal_flour_starch_milk_preparations_and_products* son más abundates que las españolas en esa misma categoría.

- - -

##5.- Representación de los resultados a partir de tablas y gráficas.

- - -

##6.- Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?

- - -

##7.- Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar en Python.

- - -

##Criterios de valoración

Todos los apartados son obligatorios. La ponderación de los ejercicios es la siguiente:

* Los apartados 1, 2 y 6 valen 0,5 puntos.
* Los apartados 3, 5 y 7 valen 2 puntos.
* El apartado 4 vale 2,5 puntos.

Se valorará la idoneidad de las respuestas, que deberán ser claras y completas. Las
diferentes etapas deberán justificarse y acompañarse del código correspondiente.

También se valorará la síntesis y claridad, a través del uso de comentarios, del código
resultante, así como la calidad de los datos finales analizados.

- - -
- - -
