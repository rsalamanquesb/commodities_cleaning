---
title: "Práctica 2 - Limpieza y validación de datos"
author: "Rubén Salamanqués y Ricardo Pardo"
date: "10 de junio de 2018"
output:
  html_document:
    toc: true
    #theme: united
    df_print: paged
    
#csl: apa.csl
#bibliography: bibliography.bib    
---
- - -

<style>
body {text-align: justify}
</style>

```{r setup, include=FALSE}
library('knitr')
library('kableExtra')
library('stringr')

knitr::opts_chunk$set(echo = TRUE)
```

##Introducción

El objetivo de esta actividad será el tratamiento de un dataset, que puede ser el creado en la práctica 1 o bien cualquier dataset libre disponible en Kaggle (https://www.kaggle.com). Algunos ejemplos de dataset con los que podéis trabajar son:

* Red Wine Quality (https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)
* Titanic: Machine Learning from Disaster (https://www.kaggle.com/c/titanic)
* Predict Future Sales (https://www.kaggle.com/c/competitive-data-sciencepredict-future-sales/).

Los últimos dos ejemplos corresponden a competiciones activas de Kaggle de manera que, opcionalmente, podríais aprovechar el trabajo realizado durante la práctica para entrar en alguna de estas competiciones.

Para llevar a cabo el trabajo de esta práctica, hemos escogido el dataset "Global Commodity Trade Statistics", que se encuentra disponible en: https://www.kaggle.com/unitednations/global-commodity-trade-statistics/data


A continuación, siguiendo las principales etapas de un proyecto analítico, las diferentes tareas a realizar (y justificar) son las siguientes:


- - -

##1.- Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?

Se trata de un dataset originalmente publicado por "United Nations Statistics Division" en la página de UNData (http://data.un.org/Explorer.aspx). Los términos de uso dicen que todos los datos y metadatos provenientes de la página de UNData pueden ser utilizados sin coste y pueden ser copiados libremente y distribuidos posteriormente siempre y cuando se cite a UNdata como fuente original.

El dataset contiene información sobre flujos de exportación e importación de animales y productos de consumo de países del mundo durante un periodo de 30 años.

Debido al gran volumen que esto supone se va a trabajar con un subconjunto que comprende el periodo entre los años 2006 y 2016.

A partir de estos datos se pueden llevar a cabo estudios sobre los flujos de exportaciones e importaciones de los países en un periodo en el que se ha producido una de las mayores crisis a nivel mundial del último siglo. Las preguntas a las que se pretende dar respuesta son las siguientes:

  * ¿Se ha reducido el volumen comercial (en $) en este periodo para los países del G20?

  * ¿Cuál será la previsión de exportación de la categoría más exportada por España?

  * ¿El mercado chino ha resistido mejor la crisis en comparación con el estadounidense?

Las columnas del dataset son las siguientes:

  * **country_or_area:** País o zona geográfica.
  * **year:** Año de la transacción.
  * **comm_code:** Código de la mercancía.
  * **commodity:** Descripción de la mercancía.
  * **flow:** Flujo de la transacción (importación/exportación/re-importación/re-exportación).
  * **trade_usd:** Precio de la transacción (en dólares estadounidenses).
  * **weight_kg:** Peso en kilogramos.
  * **quantity_name:** Magnitud de la cantidad.
  * **quantity:** Cantidad
  * **category:** Categoría genérica de la transacción

- - -

##2.- Integración y selección de los datos de interés a analizar

Como el archivo original es muy grande, se ha preparado una versión que contiene únicamente los datos a partir del año 2006. Con el siguiente código se carga dicho archivo.

```{r, include=TRUE, echo=FALSE, results='hold'}

vpath_rsb1 = "C:/Users/Ruben/Google Drive/UOC/2_Tipología y Ciclo de Vida de los Datos/Prácticas/Práctica2/commodity_trade_statistics_data_2006.csv"

vpath_rsb2 = "C:/Users/Rsb/Google Drive/UOC/2_Tipología y Ciclo de Vida de los Datos/Prácticas/Práctica2/commodity_trade_statistics_data_2006.csv"

vpath_rpc1 = "C:/Users/rpc/Documents/Google Drive/Máster Data Science/2017-2018 2ndo semestre/Tipología y ciclo de vida de los datos/Práctica 2 - Limpieza y validación de datos/commodity_trade_statistics_data_2006.csv"

vpath_rpc2 = "C:/Users/rpc/Documents/Google Drive/Máster Data Science/2017-2018 2ndo semestre/Tipología y ciclo de vida de los datos/Práctica 2 - Limpieza y validación de datos/commodity_trade_statistics_data_2006.csv"

vpath = vpath_rpc2
```

```{r, results='hold'}
# lectura del fichero con datos a partir de 2006
cmm_trade_data = read.csv(vpath, head=TRUE, sep = ",")
```

```{r, include=TRUE, echo=FALSE, results='hold'}
#borrado de variables
rm(vpath, vpath_rsb1, vpath_rsb2, vpath_rpc1, vpath_rpc2)
```

Con esto se obtiene un conjunto de datos con las características explicadas anteriormente. Sin embargo, para responder a las cuestiones planteadas, tan solo es necesario trabajar con las siguientes columnas.


  * **country_or_area:** Para hacer análisis por países.
  * **year:** Para estudiar la evolución temporal.
  * **comm_code:** Para corregir datos ausentes en otros campos.
  * **flow:** Para estudiar solo ciertos flujos.
  * **trade_usd:** Para analizar los volúmenes de las transacciones.
  * **category:** Para agrupar por categorías.
  
Además, se va a trabajar únicamente con los países del G-20 (incluyendo a España en lugar de la UE en el último puesto).

El siguiente código en R proporciona un subconjunto de los datos cargados que cumple los requisitos expuestos.

```{r, results='hold'}
G_20_data <- as.data.frame(cmm_trade_data[grep("Argentina|Australia|Brazil|Canada|China|France|Germany|India|Indonesia|Italy|Japan|Mexico|Russia|Saudi Arabia|South Africa|Spain|Rep. of Korea|Turkey|United Kingdom|USA", cmm_trade_data$country_or_area),
                    c("country_or_area",
                      "year",
                      "comm_code",
                      "flow",
                      "trade_usd",
                      "category")])
```

- - -

##3.- Limpieza de los datos

Obtenemos un resumen de los datos para llevar a cabo un análisis previo:
```{r, include=TRUE, echo=TRUE, results='hold'}
#resumen de los datos
summary(G_20_data)
```

###3.1.- ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos?

Según hemos definido anteriormente, las columnas con las que vamos a trabajar son las que enumeramos a continuación, se trata de todas las columnas del dataset "G_20_data":
  
  * **country_or_area:** Para hacer análisis por países.
  * **year:** Para estudiar la evolución temporal.
  * **comm_code:** Para corregir datos ausentes en otros campos.
  * **flow:** Para estudiar solo ciertos flujos.
  * **trade_usd:** Para analizar los volúmenes de las transacciones.
  * **category:** Para agrupar por categorías.

Por tanto, estudiaremos la existencia de ceros o elementos vacíos en estas columnas:

```{r, include=TRUE, echo=TRUE, results='hold'}
# Números de valores desconocidos por campo
sapply(G_20_data, function(x) sum(is.na(x) | x==0))
```

Vemos que ninguna de las columnas contiene ceros ni tampoco valores 'NA', por tanto no tendremos que tratar estos valores. A pesar de no tener que tratar estos valores, vamos analizar en detalle las variables **comm_code** y **category** ya que hemos visto que contienen valores especiales:
 
```{r, include=TRUE, echo=TRUE, results='hold'}
## Analizamos la columna comm_code
print("Valores de comm_code:")
summary(G_20_data$comm_code)
```

Los valores especiales que encontramos son los siguientes:    

  + "999999": que significa "Commodities not specified according to kind"
  
  + "TOTAL": representa, por país y año, el total en USD de cada uno de los posibles flujos (exportaciones, importaciones, re-exportaciones y re-importaciones).
  
Tendremos en cuenta estos valores en los puntos posteriores ya que no hemos de tratarlos como al resto de observaciones.

 
```{r, include=TRUE, echo=TRUE, results='hold'}
## Analizamos la columna category
print("Valores de category:")
summary(G_20_data$category)
```

Vemos que para el caso de la variable "category", existe un valor especial, se trata de "all_commodities", este valor se corresponde con observaciones en las que "comm_code=TOTAL", por tanto, trataremos estos casos de forma especial.

```{r, include=FALSE, echo=FALSE, results='hold'}
## Borramos las variables auxiliares utilizadas
rm(aux, aux_n0)
```

###3.2.- Identificación y tratamiento de valores extremos

Dentro del conjunto de datos que se está utilizando, el único dato sobre el que tiene sentido realizar un estudio sobre valores extremos es *trade_usd*.

Como se acaba de ver en el punto anterior, el conjunto de datos cuenta con filas sumatorio (comm_code= TOTAL o 999999). Estas filas no son válidas para realizar los análisis propuestos con lo que se procede a limpiar el conjunto de datos eliminándolas.

```{r, include=TRUE, results='hold'}

G_20_data <- G_20_data[G_20_data$comm_code != "999999" &
                       G_20_data$comm_code != "TOTAL",]

```

Una manera de visualizar los valores extremos es mediante un diagrama de caja o *boxplot*. Con el siguiente código se obtiene este diagrama.

```{r, include=TRUE, results='hold'}

par(mar=c(7,4,7,4)) #Márgenes del gráfico

boxplot(G_20_data$trade_usd, 
       main="Dispersión de trade_usd",
       xlab="Dólares estadounidenses",
       col="skyblue", #color de relleno
       boxcol="skyblue", #color de la caja
       outcol="salmon", #color de los valores extremos
       horizontal=TRUE)

```

Se puede observar que el gráfico obtenido no es legible. Esto se debe a que el campo analizado tiene un rango de valores muy grande. En el conjunto de datos se registran transacciones de varios cientos de miles de millones de dólares así como otras de tan solo decenas.

En realidad, cualquiera de estos valores es un valor válido con lo que no se considera ningún registro como valor extremo.

Por último se muestra la distribución de valores de *trade_usd* para apreciar la cantidad de datos que exiten a lo largo de todo el rango de la variable.

```{r, include=TRUE, results='hold'}

#cat("Valores atípicos para trade_usd:", boxplot.stats(G_20_data[,"trade_usd"])$out)
plot(G_20_data$trade_usd, 
     xlab="Valores de trade_usd",
     ylab="Fila",
     col="skyblue"
     )

```

- - -

##4.- Análisis de los datos

En este punto se llevará a cabo un segundo análisis de los datos para conocerlos con un mayor grado de detalle antes de comenzar con el análisis final, el cual nos llevará a la resolución de las preguntas planteadas en el punto 1.

###4.1.- Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar)

En el punto 2, tras un análisis preliminar, hemos definido el conjunto de datos de trabajo mediante una preselección del conjunto original, seleccionando únicamente las variables del conjunto original que hemos estimado que iban a ser necesarias. A partir de dicho conjunto de datos, "G_20_data", vamos a analizar qué variables serán necesarias para dar respuesta a cada una de las preguntas planteadas:

  1. ¿Se ha reducido el volumen comercial (en $) en este periodo para los países del G20?
    
    * Para dar respuesta a esta primera pregunta utilizaremos la variable **'country_or_area'**, **'year'**, **'trade_usd'**, .
    
```{r, include=TRUE, results='hold'}
G_20_data.I <- G_20_data[c("country_or_area","year","trade_usd")]
```


  2. ¿Cuál será la previsión de exportación de la categoría más exportada por España?
    
    * En cuanto a la segunda pregunta planteada, para resolverla, necesitaremos las columnas: **'country_or_area'**, **'year'**, **'flow'**, **'trade_usd'** y **'category'**.

```{r, include=TRUE, results='hold'}
G_20_data.II <- G_20_data[c("country_or_area","year","flow","trade_usd","category")]
```

  3. ¿El mercado chino ha resistido mejor la crisis en comparación con el estadounidense?
    
    * Para resolver la tercera cuestión, utilizaremos las siguientes columnas del conjunto de datos: **'country_or_area'**, **'year'**, **'flow'** y **'trade_usd'**.

```{r, include=TRUE, results='hold'}
G_20_data.III <- G_20_data[c("country_or_area","year","flow","trade_usd")]
```

###4.2.- Comprobación de la normalidad y homogeneidad de la varianza

A continuación, procederemos a analizar la normalidad y hogeneidad de la varianza de los valores que utilizaremos en el estudio.

#### Comprobación de normalidad

Necesitamos comprobar si los valores que toman las variables cuantitativas se ajustan a una distribución normal, para ello podemos utilizar alguno de los test disponibles en las librerías "normtest", "nortest" y "moments". En nuestro caso, utilizaremos la prueba de normalidad de **Anderson-Darling**, disponible en la librería "nortest".

Estableceremos un nivel de significación $\alpha = 0.05$, una vez aplicado el test, si obtenemos un $p-valor$ superior a dicho nivel de significación, se considerará que la variable sigue una distribución normal.

```{r, include=TRUE, results='hold'}
# para llevar a cabo este test, utilizaremos la librería normtest:
library(nortest)

alpha = 0.05

# llevamos a cabo el test de Anderson-Darling para la columna "trade_usd"
p_valor <- ad.test(G_20_data[,5])$p.value

paste("Obtenemos un p-valor de: ",p_valor)

if (p_valor > alpha){
  print("La variable 'trade_usd' sí sigue una distribución normal")
}else{
  print("La variable 'trade_usd' no sigue una distribución normal")
}
```

#### Homogeneidad de la varianza

A continuación, estudiaremos la homogeneidad de varianzas en cuanto a los diferentes países, para ello aplicaremos el test de **Fligner-Killen**. La hipótesis nula representa que las varianzas son iguales.

Para el estudio de la homogeneidad de la varianza, estableceremos el mismo nivel de significación que para el estudio de la normalidad: $\alpha = 0.05$

```{r, include=TRUE, results='hold'}

flt <- fligner.test(trade_usd ~ country_or_area, data=G_20_data)

print(flt)

p_valor <- flt$p.value

if (p_valor > alpha){
  print("Las muestras sí son homogéneas")
}else{
  print("Las muestras no son homogéneas")
}

```

###4.3.- Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc.

- - -

##5.- Representación de los resultados a partir de tablas y gráficas

- - -

##6.- Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?

- - -

##7.- Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar en Python.

- - -

##Criterios de valoración

Todos los apartados son obligatorios. La ponderación de los ejercicios es la siguiente:

* Los apartados 1, 2 y 6 valen 0,5 puntos.
* Los apartados 3, 5 y 7 valen 2 puntos.
* El apartado 4 vale 2,5 puntos.

Se valorará la idoneidad de las respuestas, que deberán ser claras y completas. Las
diferentes etapas deberán justificarse y acompañarse del código correspondiente.

También se valorará la síntesis y claridad, a través del uso de comentarios, del código
resultante, así como la calidad de los datos finales analizados.

- - -
- - -
